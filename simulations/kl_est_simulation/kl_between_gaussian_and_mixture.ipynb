{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dis\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dis.Normal(loc=3, scale=1)\n",
    "\n",
    "r1 = dis.Normal(loc=1, scale=1.5)\n",
    "r2 = dis.Normal(loc=2, scale=1.5)\n",
    "r3 = dis.Normal(loc=3, scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_pdf(x):\n",
    "\treturn (r1.log_prob(x).exp() + r2.log_prob(x).exp() + r3.log_prob(x).exp()) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3467)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = q.sample(sample_shape=(5000,))\n",
    "\n",
    "(q.log_prob(x) - mixture_pdf(x).log()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0166)\n",
      "tensor(0.3499)\n",
      "tensor(0.1277)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4981)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0.0\n",
    "for r in [r1, r2, r3]:\n",
    "\tprint(dis.kl.kl_divergence(q, r))\n",
    "\ttotal += dis.kl.kl_divergence(q, r) / 3\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5 * (np.log(1.5 ** 2) - np.log(1.) - 1. + (1. / 1.5) ** 2 + 1. ** 2 / 1.5 ** 2)).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(711)\n",
    "\n",
    "mu_q = torch.rand(4)\n",
    "logvar_q = torch.rand(4)\n",
    "q = dis.MultivariateNormal(mu_q, logvar_q.exp().diag())\n",
    "\n",
    "mu_rs = torch.rand(3, 4)\n",
    "logvar_rs = torch.rand(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_pdf(x):\n",
    "\tr1 = dis.MultivariateNormal(mu_rs[0], logvar_rs[0].exp() * torch.eye(4))\n",
    "\tr2 = dis.MultivariateNormal(mu_rs[1], logvar_rs[1].exp() * torch.eye(4))\n",
    "\tr3 = dis.MultivariateNormal(mu_rs[2], logvar_rs[2].exp() * torch.eye(4))\n",
    "\treturn (\n",
    "\t\tr1.log_prob(x).exp() \n",
    "\t\t+ r2.log_prob(x).exp() \n",
    "\t\t+ r3.log_prob(x).exp()\n",
    "\t) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2921)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = q.sample(sample_shape=(5000,))\n",
    "logr =  mixture_pdf(x).log() - q.log_prob(x)\n",
    "( - logr ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2470)\n",
      "tensor(0.2983)\n",
      "tensor(0.4409)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3287)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0.0\n",
    "for i in range(3):\n",
    "\tr = dis.MultivariateNormal(mu_rs[i], logvar_rs[i].exp() * torch.eye(4))\n",
    "\tprint(dis.kl.kl_divergence(q, r))\n",
    "\ttotal += dis.kl.kl_divergence(q, r) / 3\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both univariate and multivariate cases, `KL(N0 || Mix of N) < Mix of KL(N0 || N)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check `kl_ub_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_mvn(mu1, mu2, logvar_diag1, logvar_diag2):\n",
    "    m = len(mu1)\n",
    "    return 0.5 * (\n",
    "        logvar_diag2.sum()\n",
    "        - logvar_diag1.sum()\n",
    "        - m\n",
    "        + torch.sum(logvar_diag1.exp() / logvar_diag2.exp())\n",
    "        + (mu2 - mu1).T @ logvar_diag2.exp().diag().inverse() @ (mu2 - mu1)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4603)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = dis.MultivariateNormal(\n",
    "\tloc=torch.tensor((1.0, 0.0, 1.0)),\n",
    "\tcovariance_matrix=torch.diag(torch.tensor((2.0, 2.0, 2.0)))\n",
    ")\n",
    "\n",
    "p = dis.MultivariateNormal(\n",
    "\tloc=torch.tensor((2.0, 2.0, 2.0)),\n",
    "\tcovariance_matrix=torch.diag(torch.tensor((1.0, 1.0, 1.0)))\n",
    ")\n",
    "\n",
    "dis.kl.kl_divergence(q, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.460279229160082"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * (np.log(1/8) - 3 + 6 + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mu1 = torch.rand(3)\n",
    "mu2 = torch.rand(3)\n",
    "logvar_diag1 = torch.rand(3)\n",
    "logvar_diag2 = torch.rand(1) * torch.ones(3)\n",
    "\n",
    "# mu1 = torch.tensor((1.0, 0.0, 1.0))\n",
    "# mu2 = torch.tensor((2.0, 2.0, 2.0))\n",
    "# logvar_diag1 = torch.tensor((2.0, 2.0, 2.0)).log()\n",
    "# logvar_diag2 = torch.tensor((1.0, 1.0, 1.0)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2020)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.kl.kl_divergence(\n",
    "\tdis.MultivariateNormal(mu1, logvar_diag1.exp().diag()),\n",
    "\tdis.MultivariateNormal(mu2, logvar_diag2.exp().diag())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/g2lrtrss1ns2qlwbvzmvtgzm0000gn/T/ipykernel_30513/2724422101.py:8: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3575.)\n",
      "  + (mu2 - mu1).T @ logvar_diag2.exp().diag().inverse() @ (mu2 - mu1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2020)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_mvn(\n",
    "\tmu1,\n",
    "\tmu2,\n",
    "\tlogvar_diag1,\n",
    "\tlogvar_diag2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct here! but better off using dis.kl.kl_divergence\n",
    "def kl_ub_loss(mu_minor, logvar_minor, mu_major, logvar_major, reduce: bool):\n",
    "    \"\"\"Calculate KL loss's lower bound using another Jensen's Inequality.\"\"\"\n",
    "    B, m = mu_minor.shape\n",
    "    N, _ = mu_major.shape\n",
    "\n",
    "    mu_minor = mu_minor.T[None, :, :].repeat(N, 1, 1)\n",
    "    mu_major = mu_major[:, :, None].repeat(1, 1, B)\n",
    "    kl_variant = (\n",
    "        0.5 * ((mu_minor - mu_major) ** 2).sum(dim=1).mean(dim=0) / logvar_major.exp()\n",
    "    )\n",
    "\n",
    "    kl_invariant = 0.5 * (\n",
    "        m * logvar_major\n",
    "        - logvar_minor.sum(dim=1)\n",
    "        - m\n",
    "        + logvar_minor.exp().sum(dim=1) / logvar_major.exp()\n",
    "    )\n",
    "    \n",
    "    kl = kl_invariant + kl_variant\n",
    "\n",
    "    return kl.mean() if reduce else kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2020])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_ub_loss(\n",
    "\tmu1.view(1,-1),\n",
    "\tlogvar_diag1.view(1,-1),\n",
    "\tmu2.view(1,-1),\n",
    "\tlogvar_diag2[0],\n",
    "\treduce=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "B = 16\n",
    "N = 1024\n",
    "\n",
    "mu_minor = torch.rand(B, 4)\n",
    "logvar_minor = torch.rand(B, 4)\n",
    "\n",
    "mu_major = torch.rand(1024, 4)\n",
    "logvar_major = torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6279, 0.4496, 0.6519, 0.5348, 0.7484, 0.8422, 0.5366, 0.5087, 0.3609,\n",
       "        0.6599, 0.5624, 0.4689, 0.7133, 0.4305, 0.5825, 0.5916])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mu_minor = mu_minor[None, :, :].repeat(1024, 1, 1)\n",
    "_logvar_minor = logvar_minor[None, :, :].repeat(1024, 1, 1)\n",
    "_mu_major = mu_major[:, None, :].repeat(1, B, 1)\n",
    "dis.kl.kl_divergence(\n",
    "\tdis.MultivariateNormal(_mu_minor, torch.diag_embed(_logvar_minor.exp())),\n",
    "\tdis.MultivariateNormal(_mu_major, logvar_major.exp() * torch.eye(4))\n",
    ").mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6279, 0.4496, 0.6519, 0.5348, 0.7484, 0.8422, 0.5366, 0.5087, 0.3609,\n",
       "        0.6599, 0.5624, 0.4689, 0.7133, 0.4305, 0.5825, 0.5916])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_mat = torch.zeros(N, B)\n",
    "for i in range(B):\n",
    "\tfor j in range(N):\n",
    "\t\tkl_mat[j, i] = kl_mvn(mu_minor[i], mu_major[j], logvar_minor[i], logvar_major * torch.ones(4))\n",
    "\n",
    "kl_mat.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6279, 0.4496, 0.6519, 0.5348, 0.7484, 0.8422, 0.5366, 0.5087, 0.3609,\n",
       "        0.6599, 0.5624, 0.4689, 0.7133, 0.4305, 0.5825, 0.5916])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_ub_loss(mu_minor, logvar_minor, mu_major, logvar_major, reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0.3322, 0.1379, 0.3092, 0.3422, 0.3024, 0.4357, 0.2025, 0.2606, 0.1442,\n",
       "        0.3574, 0.3085, 0.1506, 0.3976, 0.1067, 0.3104, 0.2951]),\n",
       "indices=tensor([837,  82, 300, 212, 133, 470, 216,  54, 542, 537, 206, 417, 282, 509,\n",
       "        109, 251]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_mat.min(dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
