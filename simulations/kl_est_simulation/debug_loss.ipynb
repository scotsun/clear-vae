{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dis.Normal(loc=3, scale=1)\n",
    "\n",
    "r1 = dis.Normal(loc=1, scale=1.5)\n",
    "r2 = dis.Normal(loc=2, scale=1.5)\n",
    "r3 = dis.Normal(loc=3, scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_pdf(x):\n",
    "\treturn (r1.log_prob(x).exp() + r2.log_prob(x).exp() + r3.log_prob(x).exp()) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3866)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = q.sample(sample_shape=(5000,))\n",
    "\n",
    "(q.log_prob(x) - mixture_pdf(x).log()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0166)\n",
      "tensor(0.3499)\n",
      "tensor(0.1277)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4981)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0.0\n",
    "for r in [r1, r2, r3]:\n",
    "\tprint(dis.kl.kl_divergence(q, r))\n",
    "\ttotal += dis.kl.kl_divergence(q, r) / 3\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3499"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5 * (np.log(1.5 ** 2) - np.log(1.) - 1. + (1. / 1.5) ** 2 + 1. ** 2 / 1.5 ** 2)).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(711)\n",
    "\n",
    "mu_q = torch.rand(2, 4)\n",
    "logvar_q = torch.rand(2, 4)\n",
    "q = dis.MultivariateNormal(mu_q, logvar_q.exp().diag_embed())\n",
    "\n",
    "mu_rs = torch.rand(3, 4)\n",
    "logvar_r = torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_pdf(x):\n",
    "\tr1 = dis.MultivariateNormal(mu_rs[0], logvar_r.exp() * torch.eye(4))\n",
    "\tr2 = dis.MultivariateNormal(mu_rs[1], logvar_r.exp() * torch.eye(4))\n",
    "\tr3 = dis.MultivariateNormal(mu_rs[2], logvar_r.exp() * torch.eye(4))\n",
    "\treturn (\n",
    "\t\tr1.log_prob(x).exp() \n",
    "\t\t+ r2.log_prob(x).exp() \n",
    "\t\t+ r3.log_prob(x).exp()\n",
    "\t) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = q.sample(sample_shape=(5000,))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3155, 0.3018])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q.log_prob(x) - mixture_pdf(x).log()).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3155, 0.3018])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q.log_prob(x) - dis.MultivariateNormal(mu_rs, logvar_r.exp() * torch.eye(4)).log_prob(\n",
    "    x[:,:, None, :].repeat(1, 1, 3, 1)\n",
    ").exp().mean(dim=2).log()).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3086)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr =  mixture_pdf(x).log() - q.log_prob(x)\n",
    "( - logr ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3710, 0.3290])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0.0\n",
    "for i in range(3):\n",
    "\tr = dis.MultivariateNormal(mu_rs[i], logvar_r.exp() * torch.eye(4))\n",
    "\t# print(dis.kl.kl_divergence(q, r))\n",
    "\ttotal += dis.kl.kl_divergence(q, r) / 3\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem found: exponetiate logvar_major twice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mg_prior_no_loop(z, mu_major, logvar_major):\n",
    "    # note: z.shape = (T, B, m), mu_major.shape = (N, m)\n",
    "    N, m = mu_major.shape\n",
    "    r_dist = dis.MultivariateNormal(mu_major, logvar_major.exp() * torch.eye(m))   \n",
    "\n",
    "    z = z[:, :, None, :].repeat(1, 1, N, 1)\n",
    "    r_logpdf = r_dist.log_prob(z)\n",
    "    \n",
    "    dims = len(r_logpdf.shape)\n",
    "    log_prior_pdf = r_logpdf.exp().mean(dim=dims-1).log()\n",
    "    # output.shape = (T, B)\n",
    "    return log_prior_pdf\n",
    "\n",
    "\n",
    "def kl_estimated_loss1(mu_minor, logvar_minor, mu_major, logvar_major, T=30):\n",
    "    q = dis.MultivariateNormal(mu_minor, logvar_minor.exp().diag_embed())\n",
    "    z = q.sample(sample_shape=(T,))\n",
    "    # z.shape is (T, B, m)\n",
    "    log_ratio = log_mg_prior_no_loop(z, mu_major, logvar_major) - q.log_prob(z)\n",
    "    # log_ratio.shape = (T, B)\n",
    "    # k3 defined in http://joschu.net/blog/kl-approx.html\n",
    "    kl_est = ((log_ratio.exp() - 1) - log_ratio).mean(dim=0)\n",
    "    \n",
    "    return kl_est\n",
    "\n",
    "\n",
    "def kl_estimated_loss2(mu_minor, logvar_minor, mu_major, logvar_major, T=30):\n",
    "    q = dis.MultivariateNormal(mu_minor, logvar_minor.exp().diag_embed())\n",
    "    z = q.sample(sample_shape=(T,))\n",
    "    # z.shape is (T, B, m)\n",
    "    log_ratio = q.log_prob(z) - log_mg_prior_no_loop(z, mu_major, logvar_major)\n",
    "    # log_ratio.shape = (T, B)\n",
    "    kl_est = log_ratio.mean(dim=0)\n",
    "    \n",
    "    return kl_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2366, 0.1561])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_estimated_loss1(mu_q, logvar_q, mu_rs, logvar_r, T=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2863, 0.3537])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_estimated_loss2(mu_q, logvar_q, mu_rs, logvar_r, T=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both univariate and multivariate cases, `KL(N0 || Mix of N) < Mix of KL(N0 || N)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check `kl_ub_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_mvn(mu1, mu2, logvar_diag1, logvar_diag2):\n",
    "    m = len(mu1)\n",
    "    return 0.5 * (\n",
    "        logvar_diag2.sum()\n",
    "        - logvar_diag1.sum()\n",
    "        - m\n",
    "        + torch.sum(logvar_diag1.exp() / logvar_diag2.exp())\n",
    "        + (mu2 - mu1).T @ logvar_diag2.exp().diag().inverse() @ (mu2 - mu1)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4603)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = dis.MultivariateNormal(\n",
    "\tloc=torch.tensor((1.0, 0.0, 1.0)),\n",
    "\tcovariance_matrix=torch.diag(torch.tensor((2.0, 2.0, 2.0)))\n",
    ")\n",
    "\n",
    "p = dis.MultivariateNormal(\n",
    "\tloc=torch.tensor((2.0, 2.0, 2.0)),\n",
    "\tcovariance_matrix=torch.diag(torch.tensor((1.0, 1.0, 1.0)))\n",
    ")\n",
    "\n",
    "dis.kl.kl_divergence(q, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.460279229160082"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * (np.log(1/8) - 3 + 6 + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mu1 = torch.rand(3)\n",
    "mu2 = torch.rand(3)\n",
    "logvar_diag1 = torch.rand(3)\n",
    "logvar_diag2 = torch.rand(1) * torch.ones(3)\n",
    "\n",
    "# mu1 = torch.tensor((1.0, 0.0, 1.0))\n",
    "# mu2 = torch.tensor((2.0, 2.0, 2.0))\n",
    "# logvar_diag1 = torch.tensor((2.0, 2.0, 2.0)).log()\n",
    "# logvar_diag2 = torch.tensor((1.0, 1.0, 1.0)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2020)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.kl.kl_divergence(\n",
    "\tdis.MultivariateNormal(mu1, logvar_diag1.exp().diag()),\n",
    "\tdis.MultivariateNormal(mu2, logvar_diag2.exp().diag())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/g2lrtrss1ns2qlwbvzmvtgzm0000gn/T/ipykernel_18322/2724422101.py:8: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3575.)\n",
      "  + (mu2 - mu1).T @ logvar_diag2.exp().diag().inverse() @ (mu2 - mu1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2020)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_mvn(\n",
    "\tmu1,\n",
    "\tmu2,\n",
    "\tlogvar_diag1,\n",
    "\tlogvar_diag2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct here! but better off using dis.kl.kl_divergence\n",
    "def kl_ub_loss(mu_minor, logvar_minor, mu_major, logvar_major, reduce: bool):\n",
    "    \"\"\"Calculate KL loss's lower bound using another Jensen's Inequality.\"\"\"\n",
    "    B, m = mu_minor.shape\n",
    "    N, _ = mu_major.shape\n",
    "\n",
    "    mu_minor = mu_minor.T[None, :, :].repeat(N, 1, 1)\n",
    "    mu_major = mu_major[:, :, None].repeat(1, 1, B)\n",
    "    kl_variant = (\n",
    "        0.5 * ((mu_minor - mu_major) ** 2).sum(dim=1).mean(dim=0) / logvar_major.exp()\n",
    "    )\n",
    "\n",
    "    kl_invariant = 0.5 * (\n",
    "        m * logvar_major\n",
    "        - logvar_minor.sum(dim=1)\n",
    "        - m\n",
    "        + logvar_minor.exp().sum(dim=1) / logvar_major.exp()\n",
    "    )\n",
    "    \n",
    "    kl = kl_invariant + kl_variant\n",
    "\n",
    "    return kl.mean() if reduce else kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_lb_loss(mu_minor, logvar_minor, mu_major, logvar_major, min_k, reduce: bool):\n",
    "    \"\"\"Calculate KL loss's lower bound using the 'closest' k majority sample\"\"\"\n",
    "    B, m = mu_minor.shape\n",
    "    N, _ = mu_major.shape\n",
    "\n",
    "    _mu_minor = mu_minor[None, :, :].repeat(N, 1, 1)\n",
    "    _logvar_minor = logvar_minor[None, :, :].repeat(N, 1, 1)\n",
    "    _mu_major = mu_major[:, None, :].repeat(1, B, 1)\n",
    "    kl_mat = dis.kl.kl_divergence(\n",
    "        dis.MultivariateNormal(_mu_minor, torch.diag_embed(_logvar_minor.exp())),\n",
    "        dis.MultivariateNormal(_mu_major, logvar_major.exp() * torch.eye(m))\n",
    "    )\n",
    "    _, idx = torch.topk(-kl_mat, k=min_k, dim=0)\n",
    "    kl = kl_mat.gather(dim=0, index=idx)\n",
    "\n",
    "    return kl.mean() if reduce else kl.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2020])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_ub_loss(\n",
    "\tmu1.view(1,-1),\n",
    "\tlogvar_diag1.view(1,-1),\n",
    "\tmu2.view(1,-1),\n",
    "\tlogvar_diag2[0],\n",
    "\treduce=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "B = 16\n",
    "N = 1024\n",
    "\n",
    "mu_minor = torch.rand(B, 4)\n",
    "logvar_minor = torch.rand(B, 4)\n",
    "\n",
    "mu_major = torch.rand(N, 4)\n",
    "logvar_major = torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6540)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_ub_loss(mu_minor, logvar_minor, mu_major, logvar_major, reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3981)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_lb_loss(mu_minor, logvar_minor, mu_major, logvar_major, min_k=100, reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4113)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_estimated_loss1(mu_minor, logvar_minor, mu_major, logvar_major, T=1000).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC-one estimated loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dis.MultivariateNormal(mu_minor, logvar_minor.exp().diag_embed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3480)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn_like(mu_minor) * logvar_minor.exp().sqrt() + mu_minor\n",
    "log_mg_p = dis.MultivariateNormal(mu_major, logvar_major.exp() * torch.eye(4)).log_prob(\n",
    "\tz[:, None, :].repeat(1, N, 1)\n",
    ").exp().mean(dim=1).log()\n",
    "\n",
    "log_ratio = (log_mg_p - q.log_prob(z))\n",
    "((log_ratio.exp() - 1) - log_ratio).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
